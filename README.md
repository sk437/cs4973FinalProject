Dataset files are contained in app_static_task sample.

Follow instructions in instructions.txt and copy each line to install all packages - I really tried to get a one-line install working but requirements files kept having errors installing some of the necessary packages, using each line in order should work fine.

Three python files which can be run from the command line - AssignTasks, FineTuneModel and EvaluateModel. The task assignents used for my report can be found in the assignments folder.

AssignTasks - randomly picks two tasks for evaluation from each specified subtask
filename shoudl be "(x).txt", other fields are optional and specify which tasks to invlude in the assignment file, can be used in any combination
usage: python AssignTasks.py <filename> --QG --AG --IAG --CF --MM --VF

FineTuneModel - fine-tunes a model on the given tasks as specified by a file generated with AssignTasks
<assignments> should be a text file generated by AssignTasks, <model-name> is what to save the model as, and <model-type> is one of "t5" or "bart" and specifies the type of model.
usage: python FineTuneModel.py <assignments> <model-name> <model-type>

EvaluateModel - evaluates a model on the specified tasks
<model-name> and <model-type> should be the same as FineTuneModel, <model-name> should be the name of the folder/name of the checkpoint to load and evaluate
usage: python EvaluateModel.py <assignments> <model-name> <model-type>

GenerateInterestingQAPairs can also be run from the command line, but should not be needed for evaluation of models - this script is just designed to print questions and answers to which two different models had very different responses. This is run similarly to EvaluateModel, except two models and model types must be provided (in the order <model_0? <model_1> <model_type_0> <model_type_1>). Note that this script will try and find questions for which model 0 performs better than model 1, and not the other way around.
