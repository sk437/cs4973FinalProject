{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196363b8",
   "metadata": {},
   "source": [
    "Note - random split of evaluation and training tasks determined at download, divided as follows as a static option:\n",
    "Evaluation:\n",
    "1. subtask002_quoref_answer_generation  --Answer Generation(AG)                          \n",
    "2. subtask003_mctaco_question_generation_event_duration -- Question Generation(QG)\n",
    "3. subtask005_mctaco_wrong_answer_generation_event_duration -- Incorrect Answer Generation(IAG)\n",
    "4. subtask008_mctaco_wrong_answer_generation_transient_stationary -- IAG\n",
    "5. subtask022_cosmosqa_passage_inappropriate_binary -- Classification(CF)\n",
    "6. subtask033_winogrande_answer_generation -- AG\n",
    "7. subtask034_winogrande_question_modification_object -- Minimal Text Modification(MM)\n",
    "8. subtask039_qasc_find_overlapping_words -- Verification(VF)\n",
    "9. subtask040_qasc_question_generation -- QG\n",
    "10. subtask044_essential_terms_identifying_essential_words -- VF\n",
    "11. subtask045_miscellaneous_sentence_paraphrasing -- MM\n",
    "12. subtask052_multirc_identify_bad_question -- CF\n",
    "Currently randomly generates a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bea482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import torch\n",
    "# Random Split Generation - SOME FILE NAMES ARE WRONG THIS NEEDS TO BE FIXED\n",
    "categories = {'QG': ['subtask001_quoref_question_generation', \n",
    "                     'subtask003_mctaco_question_generation_event_duration', \n",
    "                     'subtask006_mctaco_question_generation_transient_stationary', \n",
    "                     'subtask009_mctaco_question_generation_event_ordering',\n",
    "                     'subtask012_mctaco_question_generation_absolute_timepoint',\n",
    "                     'subtask015_mctaco_question_generation_frequency',\n",
    "                     'subtask023_cosmosqa_question_generation',\n",
    "                     'subtask026_drop_question_generation',\n",
    "                     'subtask031_winogrande_question_generation_object',\n",
    "                     'subtask032_winogrande_question_generation_person',\n",
    "                     'subtask040_qasc_question_generation',\n",
    "                     'subtask048_multirc_question_generation',\n",
    "                     'subtask060_ropes_question_generation4'],\n",
    "              'AG': ['subtask002_quoref_answer_generation', \n",
    "                     'subtask004_mctaco_answer_generation_event_duration', \n",
    "                     'subtask007_mctaco_answer_generation_transient_stationary',\n",
    "                     'subtask010_mctaco_answer_generation_event_ordering',\n",
    "                     'subtask013_mctaco_answer_generation_absolute_timepoint',\n",
    "                     'subtask016_mctaco_answer_generation_frequency',\n",
    "                     'subtask024_cosmosqa_answer_generation',\n",
    "                     'subtask028_drop_answer_generation',\n",
    "                     'subtask033_winogrande_answer_generation',\n",
    "                     'subtask041_qasc_answer_generation',\n",
    "                     'subtask043_essential_terms_answering_incomplete_questions',\n",
    "                     'subtask047_misc_answering_science_questions',\n",
    "                     'subtask051_multirc_correct_answer_single_sentence',\n",
    "                     'subtask054_multirc_write_correct_answer',\n",
    "                     'subtask058_multirc_question_answering',\n",
    "                     'subtask061_ropes_answer_generation4'],\n",
    "              'IAG': ['subtask005_mctaco_wrong_answer_generation_event_duration', \n",
    "                      'subtask008_mctaco_wrong_answer_generation_transient_stationary',\n",
    "                      'subtask011_mctaco_wrong_answer_generation_event_ordering',\n",
    "                      'subtask014_mctaco_wrong_answer_generation_absolute_timepoint',\n",
    "                      'subtask017_mctaco_wrong_answer_generation_frequency',\n",
    "                      'subtask025_cosmosqa_incorrect_answer_generation',\n",
    "                      'subtask042_qasc_incorrect_option_generation',\n",
    "                      'subtask055_multirc_write_incorrect_answer'],\n",
    "              'CF': ['subtask018_mctaco_temporal_reasoning_presence',\n",
    "                     'subtask019_mctaco_temporal_reasoning_category',\n",
    "                     'subtask020_mctaco_span_based_question',\n",
    "                     'subtask021_mctaco_grammatical_logical',\n",
    "                     'subtask022_cosmosqa_passage_inappropriate_binary',\n",
    "                     'subtask027_drop_answer_type_generation',\n",
    "                     'subtask046_miscellaenous_question_typing',\n",
    "                     'subtask049_multirc_questions_needed_to_answer',\n",
    "                     'subtask050_multirc_answerability',\n",
    "                     'subtask052_multirc_identify_bad_question',\n",
    "                     'subtask056_multirc_classify_correct_answer',\n",
    "                     'subtask057_multirc_classify_incorrect_answer',\n",
    "                     ],\n",
    "              'MM': ['subtask029_winogrande_full_object',\n",
    "                     'subtask030_winogrande_full_person',\n",
    "                     'subtask034_winogrande_question_modification_object',\n",
    "                     'subtask035_winogrande_question_modification_person',\n",
    "                     'subtask036_qasc_topic_word_to_generate_related_fact',\n",
    "                     'subtask037_qasc_generate_related_fact',\n",
    "                     'subtask038_qasc_combined_fact',\n",
    "                     'subtask045_miscellaneous_sentence_paraphrasing',\n",
    "                     'subtask053_multirc_correct_bad_question',\n",
    "                     'subtask059_ropes_story_generation4'],\n",
    "              'VF': ['subtask039_qasc_find_overlapping_words',\n",
    "                     'subtask044_essential_terms_identifying_essential_words',\n",
    "                     ],\n",
    "              }\n",
    "\n",
    "# Move two random subtasks from each category into the evaluation subtasks\n",
    "trainingPrompts = copy.deepcopy(categories)\n",
    "evaluationPrompts = {'QG': [], 'AG': [], 'IAG': [], 'CF': [], 'MM': [], 'VF': []}\n",
    "for key in trainingPrompts.keys():\n",
    "    subtask = random.choice(trainingPrompts[key])\n",
    "    trainingPrompts[key].remove(subtask)\n",
    "    evaluationPrompts[key].append(subtask)\n",
    "    subtask = random.choice(trainingPrompts[key])\n",
    "    trainingPrompts[key].remove(subtask)\n",
    "    evaluationPrompts[key].append(subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f6333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block after to use preset subtasks, do not run to use random subtasks for evaluation\n",
    "trainingPrompts = copy.deepcopy(categories)\n",
    "evaluationPrompts = {'QG': ['subtask003_mctaco_question_generation_event_duration',\n",
    "                            'subtask040_qasc_question_generation'],\n",
    "                     'AG': ['subtask002_quoref_answer_generation',\n",
    "                            'subtask033_winogrande_answer_generation'],\n",
    "                     'IAG': ['subtask005_mctaco_wrong_answer_generation_event_duration',\n",
    "                             'subtask008_mctaco_wrong_answer_generation_transient_stationary'],\n",
    "                     'CF': ['subtask022_cosmosqa_passage_inappropriate_binary',\n",
    "                            'subtask052_multirc_identify_bad_question'],\n",
    "                     'MM': ['subtask034_winogrande_question_modification_object',\n",
    "                            'subtask045_miscellaneous_sentence_paraphrasing'],\n",
    "                     'VF': ['subtask039_qasc_find_overlapping_words',\n",
    "                            'subtask044_essential_terms_identifying_essential_words']}\n",
    "\n",
    "for key in trainingPrompts.keys():\n",
    "    for subtask in evaluationPrompts[key]:\n",
    "        trainingPrompts[key].remove(subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cbfa009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Encoding - add pos/neg examples later\n",
    "\n",
    "def no_examples_encoding(task, inp):\n",
    "    return f\"\"\"Definition: {task['Definition']}\n",
    "Prompt: {task['Prompt']}\n",
    "Things to Avoid: {task['Things to Avoid']}\n",
    "Emphasis&Caution: {task['Emphasis & Caution']}\n",
    "Input: {inp}\n",
    "Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64979139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from transformers import BartTokenizer, BartModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "random_number_model = (lambda **x: random.choice(['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine']))\n",
    "random_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base') #just using this so it takes the same inputs, output not important for random\n",
    "\n",
    "# Needs to be pretrained, takes a long time when untrained(might also be bad code)\n",
    "bart_model = BartModel.from_pretrained('facebook/bart-base')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# For testing a slightly better baseline than random, should look at GPT3\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba6f0145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611c65441ea042a8a339931b09f1ae5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7335 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07af514ce724f87b9c8cb154d8761a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2445 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "rename_columns() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-63709e5a6561>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mtokenized_data_bart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_data_bart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Instructions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mtokenized_data_bart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_data_bart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Outputs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0mtokenized_data_bart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: rename_columns() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Create pandas dataframe of samples, using no_examples_encoding, and process data into usable form\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "\n",
    "training_dict = {'Instructions': [], 'Outputs': []}\n",
    "testing_dict = {'Instructions': [], 'Outputs': []}\n",
    "\n",
    "for category in trainingPrompts.keys():\n",
    "    for task in trainingPrompts[category]:\n",
    "        with open('./app_static_tasks_sample/' + task + '.json') as json_file:\n",
    "            subtask = json.load(json_file)\n",
    "            for instance in subtask['Instances']:\n",
    "                string_encoding = no_examples_encoding(subtask, instance['input'])\n",
    "                training_dict['Instructions'].append(string_encoding)\n",
    "                training_dict['Outputs'].append(instance['output'])\n",
    "\n",
    "for category in evaluationPrompts.keys():\n",
    "    for task in evaluationPrompts[category]:\n",
    "        with open('./app_static_tasks_sample/' + task + '.json') as json_file:\n",
    "            subtask = json.load(json_file)\n",
    "            for instance in subtask['Instances']:\n",
    "                string_encoding = no_examples_encoding(subtask, instance['input'])\n",
    "                testing_dict['Instructions'].append(string_encoding)\n",
    "                testing_dict['Outputs'].append(instance['output'])\n",
    "                \n",
    "df_training = pd.DataFrame(training_dict)\n",
    "df_testing = pd.DataFrame(testing_dict)\n",
    "\n",
    "training_dataset = Dataset.from_pandas(df_training)\n",
    "testing_dataset = Dataset.from_pandas(df_testing)\n",
    "\n",
    "trainingDelimiter = int(len(df_training) * (3/4))\n",
    "\n",
    "# Create Dataset in huggingface acceptable format, for now using all tasks(will probably take way too long)\n",
    "for_finetuning = DatasetDict(\n",
    "    train=training_dataset.shuffle(seed=1111).select(range(trainingDelimiter)),\n",
    "    val=training_dataset.shuffle(seed=1111).select(range(trainingDelimiter, len(df_training)))\n",
    ")\n",
    "\n",
    "tokenized_data_bart = for_finetuning.map(\n",
    "    lambda example: bart_tokenizer(example['Instructions'], padding=True, truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "tokenized_data_bart = tokenized_data_bart.remove_columns([\"Instructions\"])\n",
    "tokenized_data_bart = tokenized_data_bart.rename_columns(\"Outputs\", \"labels\")\n",
    "tokenized_data_bart.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0106fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 117353\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22005\n",
      "  Number of trainable parameters = 139420416\n",
      "The following columns in the training set don't have a corresponding argument in `BartModel.forward` and have been ignored: Outputs. If Outputs are not expected by `BartModel.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 570.00 MiB (GPU 0; 6.00 GiB total capacity; 4.52 GiB already allocated; 0 bytes free; 5.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-5a49b56a7403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtrain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         )\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2539\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2540\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2541\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2542\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1236\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1238\u001b[1;33m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1239\u001b[0m             )\n\u001b[0;32m   1240\u001b[0m         \u001b[1;31m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    852\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m                         \u001b[0mlayer_head_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m                         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    855\u001b[0m                     )\n\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         )\n\u001b[0;32m    331\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs4973\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0msrc_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 570.00 MiB (GPU 0; 6.00 GiB total capacity; 4.52 GiB already allocated; 0 bytes free; 5.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# all of this needs much more work - figure out fine-tuning seq2seq models, dataset probably also needs to be processed better\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_data_bart['train'], batch_size=16)\n",
    "eval_dataloader = DataLoader(tokenized_data_bart['val'], batch_size=16)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_hf_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     # calculates the accuracy\n",
    "#     return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bart_model,\n",
    "    args=arguments,\n",
    "    train_dataset=tokenized_data_bart['train'],\n",
    "    eval_dataset=tokenized_data_bart['val'],\n",
    "    tokenizer=bart_tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a85e0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset into better form\n",
    "\n",
    "# class WritingTasksDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "#     def __init__(self, prompts):\n",
    "#         #super(WritingTasksDataset).__init__()\n",
    "#         self.entries = prompts\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.entries)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         if (torch.is_tensor(idx)):\n",
    "#             idx = idx.tolist()\n",
    "        \n",
    "#         return {'input': self.entries.iloc[idx, 0], 'output': self.entries.iloc[idx, 1]}\n",
    "\n",
    "    \n",
    "# trainingDelemiter = int(trainingData.__len__() * (3/4))\n",
    "# indecies = \n",
    "\n",
    "# trainingData = WritingTasksDataset(df_training)\n",
    "# evalData = WritingTasksDataset(df_eval)\n",
    "# testingData = WritingTasksDataset(df_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ac16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune BART (could probably be done much more efficiently)\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "# Calculate total number of prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca8367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QG complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG complete\n",
      "IAG complete\n",
      "CF complete\n",
      "MM complete\n",
      "VF complete\n",
      "{'QG': {'Random': 0.0002936802937205289, 'Bart': 0.06181900089641459}, 'AG': {'Random': 0.00019573302599379963, 'Bart': 0.06342681680708925}, 'IAG': {'Random': 0.0021857923619408425, 'Bart': 0.09186689122045627}, 'CF': {'Random': 0.0, 'Bart': 0.0}, 'MM': {'Random': 0.0003189032804070459, 'Bart': 0.03538358467041756}, 'VF': {'Random': 9.536888799123836e-05, 'Bart': 0.005405083401002882}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation(currently for models trained on no_examples_encoding)\n",
    "import numpy as np\n",
    "import json\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "models = {'Random': random_number_model, 'Bart': bart_model}\n",
    "tokenizers = {'Random': random_tokenizer, 'Bart': bart_tokenizer}\n",
    "# models = {'Random': random_number_model, 'GPT2': gpt2_model}\n",
    "# tokenizers = {'Random': random_tokenizer, 'GPT2': gpt2_tokenizer}\n",
    "scores = {}\n",
    "scorer = ROUGEScore(rouge_keys=('rougeL'))\n",
    "\n",
    "for category in evaluationPrompts.keys():\n",
    "    scoresForCategory = {}\n",
    "    for model in models.keys():\n",
    "        scoresForCategory[model] = []\n",
    "    for task in evaluationPrompts[category]:\n",
    "        with open('./app_static_tasks_sample/' + task + '.json') as json_file:\n",
    "            subtask = json.load(json_file)\n",
    "            for instance in subtask['Instances']:\n",
    "                string_encoding = no_examples_encoding(subtask, instance['input'])\n",
    "                for model in models.keys():\n",
    "                    tokenizer = tokenizers[model]\n",
    "                    inputs = tokenizer(string_encoding, return_tensors=\"pt\") # this might need to be more general\n",
    "                    if (len(inputs) >= 1024):\n",
    "                        print('ignored prompt')\n",
    "                        continue #temporary fix to avoid prompts which are too long\n",
    "                    try: #FIX THIS LATER\n",
    "                        outputs = models[model](**inputs)\n",
    "                        rgeScores = scorer(instance['output'], outputs)\n",
    "                        scoresForCategory[model].append(rgeScores['rougeL_fmeasure'].item())\n",
    "                    except:\n",
    "                        continue\n",
    "    for model in models.keys():\n",
    "        scoresForCategory[model] = sum(scoresForCategory[model]) / len(scoresForCategory[model])\n",
    "    scores[category] = scoresForCategory\n",
    "    print(category + \" complete\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848290a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4973",
   "language": "python",
   "name": "cs4973"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
